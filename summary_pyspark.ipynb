{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "source": [
    "# Spark SQL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession, SQLContext\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "# from a list of rows\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "],  schema='a long, b double, c string, d date, e timestamp')\n",
    "print(df)\n",
    "\n",
    "# from pandas dataframe\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "\n",
    "# from a RDD\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ],
      "text/html": "<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n</table>\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# when data volume goes up, we could change the setting to make the dataframe clearer\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangling with rdd\n",
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]\n",
    "rdd = spark.sparkContext.parallelize(list_p)\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "DF_ppl = spark.createDataFrame(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n|x  |age|workclass|fnlwgt|education   |educational-num|marital-status    |occupation       |relationship|race |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n|1  |25 |Private  |226802|11th        |7              |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0           |0           |40            |United-States |<=50K |\n|2  |38 |Private  |89814 |HS-grad     |9              |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0           |0           |50            |United-States |<=50K |\n|3  |28 |Local-gov|336951|Assoc-acdm  |12             |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0           |0           |40            |United-States |>50K  |\n|4  |44 |Private  |160323|Some-college|10             |Married-civ-spouse|Machine-op-inspct|Husband     |Black|Male  |7688        |0           |40            |United-States |>50K  |\n|5  |18 |?        |103497|Some-college|10             |Never-married     |?                |Own-child   |White|Female|0           |0           |30            |United-States |<=50K |\n+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+---+-----------+---------+------------+-------+------------+----------------+---------+-----------+\n|age_workclass|  ?|Federal-gov|Local-gov|Never-worked|Private|Self-emp-inc|Self-emp-not-inc|State-gov|Without-pay|\n+-------------+---+-----------+---------+------------+-------+------------+----------------+---------+-----------+\n|           17| 97|          2|       21|           2|    454|           8|               9|        2|          0|\n|           18|154|          5|       15|           4|    638|          12|              20|       14|          0|\n|           19|183|          6|       18|           0|    784|           6|              24|       29|          3|\n|           20|184|         13|       20|           2|    834|          11|              16|       33|          0|\n|           21|147|          4|       22|           0|    859|           4|              15|       44|          1|\n|           22|128|         16|       34|           0|    924|           9|              19|       47|          1|\n|           23| 76|         20|       52|           1|   1098|           9|              24|       49|          0|\n|           24| 56|         21|       56|           0|    986|          10|              35|       42|          0|\n|           25| 46|         14|       64|           0|    980|          15|              36|       40|          0|\n|           26| 38|         23|       61|           0|    922|          16|              46|       47|          0|\n|           27| 51|         25|       72|           0|    964|          16|              58|       45|          1|\n|           28| 55|         23|       74|           0|    990|          21|              72|       45|          0|\n|           29| 48|         34|       71|           0|    942|          18|              66|       43|          1|\n|           30| 36|         27|       83|           1|    984|          26|              72|       49|          0|\n|           31| 25|         34|       83|           0|   1023|          28|              79|       53|          0|\n|           32| 35|         30|       69|           0|    940|          34|              91|       54|          0|\n|           33| 36|         35|       85|           0|    992|          28|             105|       54|          0|\n|           34| 36|         29|       90|           0|    947|          40|             102|       59|          0|\n|           35| 43|         36|       66|           0|    995|          37|             112|       48|          0|\n|           36| 36|         34|       90|           0|    989|          36|             108|       55|          0|\n+-------------+---+-----------+---------+------------+-------+------------+----------------+---------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# # Data Wrangling\n",
    "# df.select('age','fnlwgt').show(5)\n",
    "# df.filter(df.age > 40).count()\n",
    "# df.groupby(\"education\").count().sort(\"count\", ascending=True).show()\n",
    "# df.groupby('marital-status').agg({'capital-gain': 'mean'}).show()\n",
    "df.crosstab('age', 'workclass').sort(\"age_workclass\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   48842|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# the exciting part! you can use dataframe wrangling and SQL at the same time\n",
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "source": [
    "# Spark MLlib"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "6fa8c4a0213b3e8e46e64ca221d4ef2f7254b1e53b83d6209b624a99d7aa7db4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}